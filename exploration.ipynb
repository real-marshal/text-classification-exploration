{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring text classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Trying out different text classification algorithms on one dataset. In the process, I use `scikit-learn` naive Bayes, logistic regression and multilayer perceptron with different kinds of text representation and end it with `spacy` model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers'))\n",
    "\n",
    "newsgroups_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting into pandas dataframe for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>y_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|&gt;The student of \"regional killings\" alias Dav...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In article &lt;1993Apr19.034517.12820@julian.uwo....</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   y  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...  10   \n",
       "1  My brother is in the market for a high-perform...   3   \n",
       "2  |>The student of \"regional killings\" alias Dav...  17   \n",
       "3  In article <1993Apr19.034517.12820@julian.uwo....   3   \n",
       "4  1)    I have an old Jasmine drive which I cann...   4   \n",
       "\n",
       "                     y_text  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(newsgroups_dataset.data, columns=('text',))\n",
    "\n",
    "df['y'] = newsgroups_dataset.target\n",
    "df['y_text'] = [newsgroups_dataset.target_names[y] for y in df['y']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y_text                  \n",
       "rec.sport.hockey            999\n",
       "soc.religion.christian      997\n",
       "rec.motorcycles             996\n",
       "rec.sport.baseball          994\n",
       "sci.crypt                   991\n",
       "rec.autos                   990\n",
       "sci.med                     990\n",
       "comp.windows.x              988\n",
       "sci.space                   987\n",
       "comp.os.ms-windows.misc     985\n",
       "sci.electronics             984\n",
       "comp.sys.ibm.pc.hardware    982\n",
       "misc.forsale                975\n",
       "comp.graphics               973\n",
       "comp.sys.mac.hardware       963\n",
       "talk.politics.mideast       940\n",
       "talk.politics.guns          910\n",
       "alt.atheism                 799\n",
       "talk.politics.misc          775\n",
       "talk.religion.misc          628\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(['y_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty balanced, good. Let's preprocess the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>y_text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>I am sure some bashers of Pens fan are pretty ...</td>\n",
       "      <td>i am sure some basher of pen fan are pretti co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>my brother is in the market for a high-perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>|&gt;The student of \"regional killings\" alias Dav...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>| &gt; The student of `` regional killing '' alia...</td>\n",
       "      <td>| &gt; the student of `` region kill '' alia davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In article &lt;1993Apr19.034517.12820@julian.uwo....</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>In article &lt; 1993Apr19.034517.12820 @ julian.u...</td>\n",
       "      <td>in articl &lt; 1993apr19.034517.12820 @ julian.uw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>1 ) I have an old Jasmine drive which I can no...</td>\n",
       "      <td>1 ) i have an old jasmin drive which i can not...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   y  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...  10   \n",
       "1  My brother is in the market for a high-perform...   3   \n",
       "2  |>The student of \"regional killings\" alias Dav...  17   \n",
       "3  In article <1993Apr19.034517.12820@julian.uwo....   3   \n",
       "4  1)    I have an old Jasmine drive which I cann...   4   \n",
       "\n",
       "                     y_text  \\\n",
       "0          rec.sport.hockey   \n",
       "1  comp.sys.ibm.pc.hardware   \n",
       "2     talk.politics.mideast   \n",
       "3  comp.sys.ibm.pc.hardware   \n",
       "4     comp.sys.mac.hardware   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  I am sure some bashers of Pens fan are pretty ...   \n",
       "1  My brother is in the market for a high-perform...   \n",
       "2  | > The student of `` regional killing '' alia...   \n",
       "3  In article < 1993Apr19.034517.12820 @ julian.u...   \n",
       "4  1 ) I have an old Jasmine drive which I can no...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  i am sure some basher of pen fan are pretti co...  \n",
       "1  my brother is in the market for a high-perform...  \n",
       "2  | > the student of `` region kill '' alia davi...  \n",
       "3  in articl < 1993apr19.034517.12820 @ julian.uw...  \n",
       "4  1 ) i have an old jasmin drive which i can not...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def lemmatize(text):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
    "\n",
    "def stem(text):\n",
    "  stemmer = PorterStemmer()\n",
    "  return ' '.join([stemmer.stem(w) for w in word_tokenize(text)])\n",
    "\n",
    "df['lemmatized'] = df['text'].apply(lemmatize)\n",
    "df['stemmed'] = df['text'].apply(stem)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some standard classifiers and see how they perform. I'm going to use naive Bayes, logistic regression and feedforward neural network as the models, and simple count BoW, TF-IDF, word2vec average and doc2vec as input representations. As neural net learning takes much more time and have more hyperparameters, let's omit it for now (and forget about dense vectors for a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.839788</td>\n",
       "      <td>0.831752</td>\n",
       "      <td>0.839788</td>\n",
       "      <td>0.984645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.837666</td>\n",
       "      <td>0.828380</td>\n",
       "      <td>0.837666</td>\n",
       "      <td>0.984089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.833952</td>\n",
       "      <td>0.825570</td>\n",
       "      <td>0.833952</td>\n",
       "      <td>0.984987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.820159</td>\n",
       "      <td>0.798335</td>\n",
       "      <td>0.820159</td>\n",
       "      <td>0.984802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.819098</td>\n",
       "      <td>0.793229</td>\n",
       "      <td>0.819098</td>\n",
       "      <td>0.984583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.809549</td>\n",
       "      <td>0.807633</td>\n",
       "      <td>0.809549</td>\n",
       "      <td>0.975277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.802653</td>\n",
       "      <td>0.797108</td>\n",
       "      <td>0.802653</td>\n",
       "      <td>0.973681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>0.768995</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>0.985709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.779431</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.968297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.787804</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.974815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.790981</td>\n",
       "      <td>0.768122</td>\n",
       "      <td>0.790981</td>\n",
       "      <td>0.965359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.780902</td>\n",
       "      <td>0.760030</td>\n",
       "      <td>0.780902</td>\n",
       "      <td>0.967215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input       vectorizer               model  accuracy  f1_macro  \\\n",
       "3         text  TfidfVectorizer  LogisticRegression  0.839788  0.831752   \n",
       "7   lemmatized  TfidfVectorizer  LogisticRegression  0.837666  0.828380   \n",
       "11     stemmed  TfidfVectorizer  LogisticRegression  0.833952  0.825570   \n",
       "2         text  TfidfVectorizer       MultinomialNB  0.820159  0.798335   \n",
       "6   lemmatized  TfidfVectorizer       MultinomialNB  0.819098  0.793229   \n",
       "1         text  CountVectorizer  LogisticRegression  0.809549  0.807633   \n",
       "5   lemmatized  CountVectorizer  LogisticRegression  0.802653  0.797108   \n",
       "10     stemmed  TfidfVectorizer       MultinomialNB  0.797347  0.768995   \n",
       "0         text  CountVectorizer       MultinomialNB  0.793103  0.779431   \n",
       "9      stemmed  CountVectorizer  LogisticRegression  0.793103  0.787804   \n",
       "4   lemmatized  CountVectorizer       MultinomialNB  0.790981  0.768122   \n",
       "8      stemmed  CountVectorizer       MultinomialNB  0.780902  0.760030   \n",
       "\n",
       "    f1_micro   auc ovr  \n",
       "3   0.839788  0.984645  \n",
       "7   0.837666  0.984089  \n",
       "11  0.833952  0.984987  \n",
       "2   0.820159  0.984802  \n",
       "6   0.819098  0.984583  \n",
       "1   0.809549  0.975277  \n",
       "5   0.802653  0.973681  \n",
       "10  0.797347  0.985709  \n",
       "0   0.793103  0.968297  \n",
       "9   0.793103  0.974815  \n",
       "4   0.790981  0.965359  \n",
       "8   0.780902  0.967215  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import re\n",
    "import functools\n",
    "\n",
    "get_class_str = lambda class_: re.search(r\"'(.*)'\", str(class_)).groups()[0].split('.')[-1]\n",
    "\n",
    "def try_models(\n",
    "    df,\n",
    "    input_columns,\n",
    "    vectorizers,\n",
    "    models,\n",
    "    model_args = {},\n",
    "    select_features_fit=lambda X, y: (X, []),\n",
    "    k_folds = None\n",
    "  ):\n",
    "  y = df['y']\n",
    "  results = []\n",
    "\n",
    "  for input_column in input_columns:\n",
    "    X = df[input_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    for Vectorizer in vectorizers:\n",
    "      vectorizer = Vectorizer(stop_words='english', max_features=100_000)\n",
    "      X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "      X_test_vectors = vectorizer.transform(X_test)\n",
    "      X_train_vectors, f_transformers = select_features_fit(X_train_vectors, y_train)\n",
    "      X_test_vectors = (\n",
    "        functools.reduce(lambda vectors, f_transformer: f_transformer.transform(vectors), f_transformers, X_test_vectors)\n",
    "          if len(f_transformers)\n",
    "          else X_test_vectors\n",
    "      )\n",
    "\n",
    "      for Model in models:\n",
    "        model = Model(**model_args.get(Model, dict()))\n",
    "\n",
    "        if not k_folds:\n",
    "          model.fit(X_train_vectors, y_train)\n",
    "          y_pred = model.predict(X_test_vectors)\n",
    "          scores = [\n",
    "            accuracy_score(y_test, y_pred),\n",
    "            f1_score(y_test, y_pred, average='macro'),\n",
    "            f1_score(y_test, y_pred, average='micro'),\n",
    "            roc_auc_score(y_test, model.predict_proba(X_test_vectors), multi_class=\"ovr\")\n",
    "          ]\n",
    "        else:\n",
    "          cv_scores = cross_validate(\n",
    "            model, X_train_vectors, y_train, cv=k_folds, scoring=('accuracy', 'f1_macro', 'f1_micro', 'roc_auc_ovr')\n",
    "          )\n",
    "          scores = [\n",
    "            f\"{cv_scores['test_accuracy'].mean().format_float_positional(precision=3, min_digits=3)}±{cv_scores['test_accuracy'].std().format_float_positional(precision=3, min_digits=3)}\",\n",
    "            f\"{cv_scores['test_f1_macro'].mean().format_float_positional(precision=3, min_digits=3)}±{cv_scores['test_f1_macro'].std().format_float_positional(precision=3, min_digits=3)}\",\n",
    "            f\"{cv_scores['test_f1_micro'].mean().format_float_positional(precision=3, min_digits=3)}±{cv_scores['test_f1_micro'].std().format_float_positional(precision=3, min_digits=3)}\",\n",
    "            f\"{cv_scores['test_roc_auc_ovr'].mean().format_float_positional(precision=3, min_digits=3)}±{cv_scores['test_roc_auc_ovr'].std().format_float_positional(precision=3, min_digits=3)}\"\n",
    "          ]\n",
    "\n",
    "        results.append([input_column, get_class_str(Vectorizer), get_class_str(Model)] + scores)\n",
    "\n",
    "  return pd.DataFrame(\n",
    "    results,\n",
    "    columns=['input', 'vectorizer', 'model', 'accuracy', 'f1_macro', 'f1_micro', 'auc ovr']\n",
    "  ).sort_values(by=['accuracy'], ascending=False)\n",
    "\n",
    "df_half = df.sample(frac=0.5)\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[CountVectorizer, TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectedly, TF-IDF outperforms CountVectorizer, but what if we first select some more useful features? As far as I understand, the IDF part of TF-IDF has a purpose that's pretty similar to what chi-squared test does (by reducing the value of words which don't matter much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.834445</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.985484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.841910</td>\n",
       "      <td>0.835853</td>\n",
       "      <td>0.841910</td>\n",
       "      <td>0.987168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.823873</td>\n",
       "      <td>0.810696</td>\n",
       "      <td>0.823873</td>\n",
       "      <td>0.982055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.822281</td>\n",
       "      <td>0.799417</td>\n",
       "      <td>0.822281</td>\n",
       "      <td>0.986040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.820690</td>\n",
       "      <td>0.800688</td>\n",
       "      <td>0.820690</td>\n",
       "      <td>0.987224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.806897</td>\n",
       "      <td>0.786673</td>\n",
       "      <td>0.806897</td>\n",
       "      <td>0.972428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.806897</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.806897</td>\n",
       "      <td>0.973705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.803183</td>\n",
       "      <td>0.799707</td>\n",
       "      <td>0.803183</td>\n",
       "      <td>0.974059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>0.781862</td>\n",
       "      <td>0.797347</td>\n",
       "      <td>0.970854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.795756</td>\n",
       "      <td>0.769527</td>\n",
       "      <td>0.795756</td>\n",
       "      <td>0.983209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.787268</td>\n",
       "      <td>0.759764</td>\n",
       "      <td>0.787268</td>\n",
       "      <td>0.966475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.782493</td>\n",
       "      <td>0.774350</td>\n",
       "      <td>0.782493</td>\n",
       "      <td>0.971129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input       vectorizer               model  accuracy  f1_macro  \\\n",
       "7   lemmatized  TfidfVectorizer  LogisticRegression  0.844032  0.834445   \n",
       "3         text  TfidfVectorizer  LogisticRegression  0.841910  0.835853   \n",
       "11     stemmed  TfidfVectorizer  LogisticRegression  0.823873  0.810696   \n",
       "6   lemmatized  TfidfVectorizer       MultinomialNB  0.822281  0.799417   \n",
       "2         text  TfidfVectorizer       MultinomialNB  0.820690  0.800688   \n",
       "4   lemmatized  CountVectorizer       MultinomialNB  0.806897  0.786673   \n",
       "5   lemmatized  CountVectorizer  LogisticRegression  0.806897  0.799401   \n",
       "1         text  CountVectorizer  LogisticRegression  0.803183  0.799707   \n",
       "0         text  CountVectorizer       MultinomialNB  0.797347  0.781862   \n",
       "10     stemmed  TfidfVectorizer       MultinomialNB  0.795756  0.769527   \n",
       "8      stemmed  CountVectorizer       MultinomialNB  0.787268  0.759764   \n",
       "9      stemmed  CountVectorizer  LogisticRegression  0.782493  0.774350   \n",
       "\n",
       "    f1_micro   auc ovr  \n",
       "7   0.844032  0.985484  \n",
       "3   0.841910  0.987168  \n",
       "11  0.823873  0.982055  \n",
       "6   0.822281  0.986040  \n",
       "2   0.820690  0.987224  \n",
       "4   0.806897  0.972428  \n",
       "5   0.806897  0.973705  \n",
       "1   0.803183  0.974059  \n",
       "0   0.797347  0.970854  \n",
       "10  0.795756  0.983209  \n",
       "8   0.787268  0.966475  \n",
       "9   0.782493  0.971129  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[CountVectorizer, TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (p := SelectPercentile(chi2, percentile=50)) and (p.fit_transform(X, y), [p])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like it doesn't help at all. Before getting rid of count vectorizers completely, let's first try to scale the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.867905</td>\n",
       "      <td>0.861966</td>\n",
       "      <td>0.867905</td>\n",
       "      <td>0.989479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.853581</td>\n",
       "      <td>0.848973</td>\n",
       "      <td>0.853581</td>\n",
       "      <td>0.988855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.847269</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.988050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.836445</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.986850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.838196</td>\n",
       "      <td>0.828103</td>\n",
       "      <td>0.838196</td>\n",
       "      <td>0.987023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.821446</td>\n",
       "      <td>0.831300</td>\n",
       "      <td>0.986780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.785146</td>\n",
       "      <td>0.787380</td>\n",
       "      <td>0.785146</td>\n",
       "      <td>0.978739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.777719</td>\n",
       "      <td>0.769298</td>\n",
       "      <td>0.777719</td>\n",
       "      <td>0.968724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.774536</td>\n",
       "      <td>0.764712</td>\n",
       "      <td>0.774536</td>\n",
       "      <td>0.968712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.774005</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.774005</td>\n",
       "      <td>0.976832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.768170</td>\n",
       "      <td>0.761433</td>\n",
       "      <td>0.768170</td>\n",
       "      <td>0.967811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.766578</td>\n",
       "      <td>0.768339</td>\n",
       "      <td>0.766578</td>\n",
       "      <td>0.976389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         input       vectorizer               model  accuracy  f1_macro  \\\n",
       "11     stemmed  TfidfVectorizer  LogisticRegression  0.867905  0.861966   \n",
       "3         text  TfidfVectorizer  LogisticRegression  0.853581  0.848973   \n",
       "7   lemmatized  TfidfVectorizer  LogisticRegression  0.851989  0.847269   \n",
       "10     stemmed  TfidfVectorizer       MultinomialNB  0.846154  0.836445   \n",
       "2         text  TfidfVectorizer       MultinomialNB  0.838196  0.828103   \n",
       "6   lemmatized  TfidfVectorizer       MultinomialNB  0.831300  0.821446   \n",
       "9      stemmed  CountVectorizer  LogisticRegression  0.785146  0.787380   \n",
       "0         text  CountVectorizer       MultinomialNB  0.777719  0.769298   \n",
       "4   lemmatized  CountVectorizer       MultinomialNB  0.774536  0.764712   \n",
       "1         text  CountVectorizer  LogisticRegression  0.774005  0.778656   \n",
       "8      stemmed  CountVectorizer       MultinomialNB  0.768170  0.761433   \n",
       "5   lemmatized  CountVectorizer  LogisticRegression  0.766578  0.768339   \n",
       "\n",
       "    f1_micro   auc ovr  \n",
       "11  0.867905  0.989479  \n",
       "3   0.853581  0.988855  \n",
       "7   0.851989  0.988050  \n",
       "10  0.846154  0.986850  \n",
       "2   0.838196  0.987023  \n",
       "6   0.831300  0.986780  \n",
       "9   0.785146  0.978739  \n",
       "0   0.777719  0.968724  \n",
       "4   0.774536  0.968712  \n",
       "1   0.774005  0.976832  \n",
       "8   0.768170  0.967811  \n",
       "5   0.766578  0.976389  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[CountVectorizer, TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, no point in using them. But scaling seems to help, especially for logistic regression accuracy and learning time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.858198</td>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.987958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.846447</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.987075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.845610</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.985988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.788329</td>\n",
       "      <td>0.773714</td>\n",
       "      <td>0.788329</td>\n",
       "      <td>0.966094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.782493</td>\n",
       "      <td>0.771113</td>\n",
       "      <td>0.782493</td>\n",
       "      <td>0.963819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.762865</td>\n",
       "      <td>0.754086</td>\n",
       "      <td>0.762865</td>\n",
       "      <td>0.959250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input       vectorizer               model  accuracy  f1_macro  \\\n",
       "3  lemmatized  TfidfVectorizer  LogisticRegression  0.861008  0.858198   \n",
       "1        text  TfidfVectorizer  LogisticRegression  0.853050  0.846447   \n",
       "5     stemmed  TfidfVectorizer  LogisticRegression  0.848276  0.845610   \n",
       "0        text  TfidfVectorizer       MultinomialNB  0.788329  0.773714   \n",
       "2  lemmatized  TfidfVectorizer       MultinomialNB  0.782493  0.771113   \n",
       "4     stemmed  TfidfVectorizer       MultinomialNB  0.762865  0.754086   \n",
       "\n",
       "   f1_micro   auc ovr  \n",
       "3  0.861008  0.987958  \n",
       "1  0.853050  0.987075  \n",
       "5  0.848276  0.985988  \n",
       "0  0.788329  0.966094  \n",
       "2  0.782493  0.963819  \n",
       "4  0.762865  0.959250  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(X), [s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how NB got much worse without feature selection. LR also took a hit. Let's try a lower smoothing parameter for naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.840003</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.987428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.840538</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.986581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.834483</td>\n",
       "      <td>0.826015</td>\n",
       "      <td>0.834483</td>\n",
       "      <td>0.987506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0        text  TfidfVectorizer  MultinomialNB  0.851989  0.840003  0.851989   \n",
       "1  lemmatized  TfidfVectorizer  MultinomialNB  0.846154  0.840538  0.846154   \n",
       "2     stemmed  TfidfVectorizer  MultinomialNB  0.834483  0.826015  0.834483   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.987428  \n",
       "1  0.986581  \n",
       "2  0.987506  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MultinomialNB],\n",
    "  model_args={\n",
    "    MultinomialNB: dict(alpha=0.5)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With smart feature selection and lower smoothing, NB is almost on par with LR. Pretty good performance, considering how much faster and simpler NB is than LR. We have to keep in mind that default LR here also uses L2 regularization. Let's take a look at LR with different kinds of regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL2</td>\n",
       "      <td>0.857294</td>\n",
       "      <td>0.853415</td>\n",
       "      <td>0.857294</td>\n",
       "      <td>0.989124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRNone</td>\n",
       "      <td>0.856233</td>\n",
       "      <td>0.853239</td>\n",
       "      <td>0.856233</td>\n",
       "      <td>0.989118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL2</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.849353</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.989918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL2</td>\n",
       "      <td>0.854642</td>\n",
       "      <td>0.849647</td>\n",
       "      <td>0.854642</td>\n",
       "      <td>0.987401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRNone</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.847506</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.989878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRNone</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.843097</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.986751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL1</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.770466</td>\n",
       "      <td>0.779841</td>\n",
       "      <td>0.978777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL1</td>\n",
       "      <td>0.773475</td>\n",
       "      <td>0.766414</td>\n",
       "      <td>0.773475</td>\n",
       "      <td>0.974276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LRL1</td>\n",
       "      <td>0.771883</td>\n",
       "      <td>0.766111</td>\n",
       "      <td>0.771883</td>\n",
       "      <td>0.970404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input       vectorizer   model  accuracy  f1_macro  f1_micro   auc ovr\n",
       "5  lemmatized  TfidfVectorizer    LRL2  0.857294  0.853415  0.857294  0.989124\n",
       "3  lemmatized  TfidfVectorizer  LRNone  0.856233  0.853239  0.856233  0.989118\n",
       "8     stemmed  TfidfVectorizer    LRL2  0.855172  0.849353  0.855172  0.989918\n",
       "2        text  TfidfVectorizer    LRL2  0.854642  0.849647  0.854642  0.987401\n",
       "6     stemmed  TfidfVectorizer  LRNone  0.853050  0.847506  0.853050  0.989878\n",
       "0        text  TfidfVectorizer  LRNone  0.848276  0.843097  0.848276  0.986751\n",
       "7     stemmed  TfidfVectorizer    LRL1  0.779841  0.770466  0.779841  0.978777\n",
       "4  lemmatized  TfidfVectorizer    LRL1  0.773475  0.766414  0.773475  0.974276\n",
       "1        text  TfidfVectorizer    LRL1  0.771883  0.766111  0.771883  0.970404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LRNone(LogisticRegression):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(penalty=None, solver='saga', **kwargs)\n",
    "\n",
    "class LRL1(LogisticRegression):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(penalty='l1', solver='saga', **kwargs)\n",
    "\n",
    "class LRL2(LogisticRegression):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(penalty='l2', solver='saga', **kwargs)\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[LRNone, LRL1, LRL2],\n",
    "  model_args={\n",
    "    LRNone: dict(max_iter=500),\n",
    "    LRL1: dict(max_iter=500),\n",
    "    LRL2: dict(max_iter=500),\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 6 mins and 500 iterations 6 out of 9 models still didn't converge. But even like this we can see how L2 regularization helps. Let's perform 5-fold cross validation to see if lemmatization and stemming has any noticable effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.835±0.011</td>\n",
       "      <td>0.829±0.012</td>\n",
       "      <td>0.835±0.011</td>\n",
       "      <td>0.986±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.835±0.001</td>\n",
       "      <td>0.824±0.002</td>\n",
       "      <td>0.835±0.001</td>\n",
       "      <td>0.985±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.833±0.011</td>\n",
       "      <td>0.822±0.013</td>\n",
       "      <td>0.833±0.011</td>\n",
       "      <td>0.985±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.832±0.01</td>\n",
       "      <td>0.827±0.01</td>\n",
       "      <td>0.832±0.01</td>\n",
       "      <td>0.986±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.832±0.008</td>\n",
       "      <td>0.827±0.009</td>\n",
       "      <td>0.832±0.008</td>\n",
       "      <td>0.985±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.825±0.007</td>\n",
       "      <td>0.814±0.008</td>\n",
       "      <td>0.825±0.007</td>\n",
       "      <td>0.985±0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input       vectorizer               model     accuracy     f1_macro  \\\n",
       "3  lemmatized  TfidfVectorizer  LogisticRegression  0.835±0.011  0.829±0.012   \n",
       "0        text  TfidfVectorizer       MultinomialNB  0.835±0.001  0.824±0.002   \n",
       "2  lemmatized  TfidfVectorizer       MultinomialNB  0.833±0.011  0.822±0.013   \n",
       "5     stemmed  TfidfVectorizer  LogisticRegression   0.832±0.01   0.827±0.01   \n",
       "1        text  TfidfVectorizer  LogisticRegression  0.832±0.008  0.827±0.009   \n",
       "4     stemmed  TfidfVectorizer       MultinomialNB  0.825±0.007  0.814±0.008   \n",
       "\n",
       "      f1_micro      auc ovr  \n",
       "3  0.835±0.011  0.986±0.002  \n",
       "0  0.835±0.001  0.985±0.002  \n",
       "2  0.833±0.011  0.985±0.002  \n",
       "5   0.832±0.01  0.986±0.002  \n",
       "1  0.832±0.008  0.985±0.001  \n",
       "4  0.825±0.007  0.985±0.001  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    "  k_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's practically no difference. Finally, let's take a look at the performance on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.87±0.001</td>\n",
       "      <td>0.868±0.001</td>\n",
       "      <td>0.87±0.001</td>\n",
       "      <td>0.99±0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.877±0.004</td>\n",
       "      <td>0.875±0.004</td>\n",
       "      <td>0.877±0.004</td>\n",
       "      <td>0.991±0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.873±0.004</td>\n",
       "      <td>0.871±0.004</td>\n",
       "      <td>0.873±0.004</td>\n",
       "      <td>0.991±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.871±0.007</td>\n",
       "      <td>0.863±0.006</td>\n",
       "      <td>0.871±0.007</td>\n",
       "      <td>0.991±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.866±0.003</td>\n",
       "      <td>0.858±0.004</td>\n",
       "      <td>0.866±0.003</td>\n",
       "      <td>0.991±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemmed</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.859±0.004</td>\n",
       "      <td>0.85±0.003</td>\n",
       "      <td>0.859±0.004</td>\n",
       "      <td>0.99±0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input       vectorizer               model     accuracy     f1_macro  \\\n",
       "5     stemmed  TfidfVectorizer  LogisticRegression   0.87±0.001  0.868±0.001   \n",
       "3  lemmatized  TfidfVectorizer  LogisticRegression  0.877±0.004  0.875±0.004   \n",
       "1        text  TfidfVectorizer  LogisticRegression  0.873±0.004  0.871±0.004   \n",
       "2  lemmatized  TfidfVectorizer       MultinomialNB  0.871±0.007  0.863±0.006   \n",
       "0        text  TfidfVectorizer       MultinomialNB  0.866±0.003  0.858±0.004   \n",
       "4     stemmed  TfidfVectorizer       MultinomialNB  0.859±0.004   0.85±0.003   \n",
       "\n",
       "      f1_micro      auc ovr  \n",
       "5   0.87±0.001     0.99±0.0  \n",
       "3  0.877±0.004    0.991±0.0  \n",
       "1  0.873±0.004  0.991±0.001  \n",
       "2  0.871±0.007  0.991±0.001  \n",
       "0  0.866±0.003  0.991±0.001  \n",
       "4  0.859±0.004   0.99±0.001  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_models(\n",
    "  df=df,\n",
    "  input_columns=['text', 'lemmatized', 'stemmed'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MultinomialNB, LogisticRegression],\n",
    "  model_args={\n",
    "    LogisticRegression: dict(max_iter=500)\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    "  k_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try using a simple neural net from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.81008</td>\n",
       "      <td>0.802017</td>\n",
       "      <td>0.81008</td>\n",
       "      <td>0.978159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier   0.81008  0.802017   0.81008   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.978159  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(10,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=50)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that's not good. Let's try to have a bit more units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.850928</td>\n",
       "      <td>0.847085</td>\n",
       "      <td>0.850928</td>\n",
       "      <td>0.988836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.850928  0.847085  0.850928   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.988836  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(50,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=25)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's almost the same as NB. Let's continue increasing neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.867±0.011</td>\n",
       "      <td>0.864±0.01</td>\n",
       "      <td>0.867±0.011</td>\n",
       "      <td>0.99±0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model     accuracy    f1_macro     f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.867±0.011  0.864±0.01  0.867±0.011   \n",
       "\n",
       "      auc ovr  \n",
       "0  0.99±0.002  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(100,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=25)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    "  k_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a bit worse than NB. Not sure, if using huge sparse vectors for the NN is a good idea, so I'll use word2vec to make dense vectors instead a bit later, but for now let's just try to increase the accuracy to beat NB and LR without using too many neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.856277</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.988086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.861538  0.856277  0.861538   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.988086  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(250,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=25)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, same as NB. The problem in comparing these results is also that I should use k-fold cross-validation but it already takes more than 5 mins so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.859947</td>\n",
       "      <td>0.856978</td>\n",
       "      <td>0.859947</td>\n",
       "      <td>0.989669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.859947  0.856978  0.859947   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.989669  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(250, 125))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=25)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1 more layer the results seem to be worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.850234</td>\n",
       "      <td>0.851989</td>\n",
       "      <td>0.987666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.851989  0.850234  0.851989   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.987666  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(500,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=25)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 mins to get 0.85 accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>auc ovr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.837919</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.986064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  input       vectorizer          model  accuracy  f1_macro  f1_micro  \\\n",
       "0  text  TfidfVectorizer  MLPClassifier  0.846154  0.837919  0.846154   \n",
       "\n",
       "    auc ovr  \n",
       "0  0.986064  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "try_models(\n",
    "  df=df_half,\n",
    "  input_columns=['text'],\n",
    "  vectorizers=[TfidfVectorizer],\n",
    "  models=[MLPClassifier],\n",
    "  model_args={\n",
    "    MLPClassifier: dict(hidden_layer_sizes=(500,))\n",
    "  },\n",
    "  select_features_fit=lambda X, y: (\n",
    "    (p := SelectPercentile(chi2, percentile=10)) and\n",
    "    (s := MaxAbsScaler()) and\n",
    "    (s.fit_transform(p.fit_transform(X, y)), [p, s])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yeah, it's much harder to train NNs than NB or LR. Finally, before moving on to using word2vec, let's make a truly correct comparison using grid search and cross validation and leave this for a day to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=5; accuracy: (test=0.821) roc_auc_ovr: (test=0.982) total time= 1.7min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=5; accuracy: (test=0.823) roc_auc_ovr: (test=0.984) total time= 1.9min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=5; accuracy: (test=0.820) roc_auc_ovr: (test=0.983) total time= 1.9min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=5; accuracy: (test=0.820) roc_auc_ovr: (test=0.983) total time= 1.6min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=5; accuracy: (test=0.812) roc_auc_ovr: (test=0.982) total time= 1.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.987) total time= 2.1min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=10; accuracy: (test=0.862) roc_auc_ovr: (test=0.989) total time= 2.0min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.989) total time= 2.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.989) total time= 1.6min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=10; accuracy: (test=0.846) roc_auc_ovr: (test=0.986) total time= 2.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=20; accuracy: (test=0.876) roc_auc_ovr: (test=0.990) total time= 3.8min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=20; accuracy: (test=0.877) roc_auc_ovr: (test=0.992) total time= 3.5min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=20; accuracy: (test=0.879) roc_auc_ovr: (test=0.992) total time= 3.2min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=20; accuracy: (test=0.876) roc_auc_ovr: (test=0.990) total time= 2.7min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=20; accuracy: (test=0.872) roc_auc_ovr: (test=0.991) total time= 2.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=40; accuracy: (test=0.886) roc_auc_ovr: (test=0.992) total time= 4.6min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=40; accuracy: (test=0.885) roc_auc_ovr: (test=0.993) total time= 4.7min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=40; accuracy: (test=0.887) roc_auc_ovr: (test=0.993) total time= 4.5min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=40; accuracy: (test=0.881) roc_auc_ovr: (test=0.991) total time= 5.2min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50,), selectpercentile__percentile=40; accuracy: (test=0.883) roc_auc_ovr: (test=0.992) total time= 5.2min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=5; accuracy: (test=0.819) roc_auc_ovr: (test=0.982) total time= 4.3min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=5; accuracy: (test=0.825) roc_auc_ovr: (test=0.985) total time= 4.5min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=5; accuracy: (test=0.825) roc_auc_ovr: (test=0.984) total time= 4.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=5; accuracy: (test=0.817) roc_auc_ovr: (test=0.982) total time= 4.8min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=5; accuracy: (test=0.819) roc_auc_ovr: (test=0.983) total time= 4.1min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=10; accuracy: (test=0.858) roc_auc_ovr: (test=0.987) total time= 6.6min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=10; accuracy: (test=0.857) roc_auc_ovr: (test=0.989) total time= 7.7min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=10; accuracy: (test=0.861) roc_auc_ovr: (test=0.989) total time= 6.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=10; accuracy: (test=0.851) roc_auc_ovr: (test=0.988) total time= 6.5min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=10; accuracy: (test=0.850) roc_auc_ovr: (test=0.986) total time= 9.2min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=20; accuracy: (test=0.881) roc_auc_ovr: (test=0.991) total time=11.2min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=20; accuracy: (test=0.880) roc_auc_ovr: (test=0.992) total time=13.1min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=20; accuracy: (test=0.881) roc_auc_ovr: (test=0.992) total time= 9.9min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=20; accuracy: (test=0.870) roc_auc_ovr: (test=0.990) total time=13.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=20; accuracy: (test=0.874) roc_auc_ovr: (test=0.991) total time=11.5min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=40; accuracy: (test=0.890) roc_auc_ovr: (test=0.992) total time=16.7min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=40; accuracy: (test=0.891) roc_auc_ovr: (test=0.994) total time=20.0min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=40; accuracy: (test=0.888) roc_auc_ovr: (test=0.993) total time=22.9min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=40; accuracy: (test=0.881) roc_auc_ovr: (test=0.992) total time=15.5min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250,), selectpercentile__percentile=40; accuracy: (test=0.884) roc_auc_ovr: (test=0.992) total time=24.5min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=5; accuracy: (test=0.827) roc_auc_ovr: (test=0.983) total time= 5.9min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=5; accuracy: (test=0.830) roc_auc_ovr: (test=0.985) total time= 7.3min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=5; accuracy: (test=0.824) roc_auc_ovr: (test=0.984) total time= 7.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=5; accuracy: (test=0.822) roc_auc_ovr: (test=0.983) total time= 7.2min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=5; accuracy: (test=0.813) roc_auc_ovr: (test=0.982) total time= 8.9min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=10; accuracy: (test=0.861) roc_auc_ovr: (test=0.988) total time=10.0min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=10; accuracy: (test=0.854) roc_auc_ovr: (test=0.988) total time=15.2min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=10; accuracy: (test=0.859) roc_auc_ovr: (test=0.989) total time=12.3min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=10; accuracy: (test=0.853) roc_auc_ovr: (test=0.988) total time=10.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.987) total time=13.0min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=20; accuracy: (test=0.878) roc_auc_ovr: (test=0.990) total time=20.2min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=20; accuracy: (test=0.875) roc_auc_ovr: (test=0.992) total time=26.5min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=20; accuracy: (test=0.879) roc_auc_ovr: (test=0.992) total time=21.4min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=20; accuracy: (test=0.868) roc_auc_ovr: (test=0.988) total time=30.9min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=20; accuracy: (test=0.874) roc_auc_ovr: (test=0.991) total time=18.7min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=40; accuracy: (test=0.892) roc_auc_ovr: (test=0.991) total time=40.5min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=40; accuracy: (test=0.882) roc_auc_ovr: (test=0.993) total time=37.4min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=40; accuracy: (test=0.886) roc_auc_ovr: (test=0.992) total time=38.2min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=40; accuracy: (test=0.869) roc_auc_ovr: (test=0.990) total time=58.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500,), selectpercentile__percentile=40; accuracy: (test=0.881) roc_auc_ovr: (test=0.990) total time=53.7min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=5; accuracy: (test=0.821) roc_auc_ovr: (test=0.981) total time=15.3min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=5; accuracy: (test=0.832) roc_auc_ovr: (test=0.986) total time= 9.4min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=5; accuracy: (test=0.824) roc_auc_ovr: (test=0.983) total time=13.4min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=5; accuracy: (test=0.817) roc_auc_ovr: (test=0.980) total time=20.9min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=5; accuracy: (test=0.817) roc_auc_ovr: (test=0.983) total time=14.6min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=10; accuracy: (test=0.841) roc_auc_ovr: (test=0.983) total time=31.1min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.989) total time=24.2min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=10; accuracy: (test=0.855) roc_auc_ovr: (test=0.989) total time=18.4min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=10; accuracy: (test=0.847) roc_auc_ovr: (test=0.986) total time=30.0min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=10; accuracy: (test=0.851) roc_auc_ovr: (test=0.987) total time=23.5min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=20; accuracy: (test=0.876) roc_auc_ovr: (test=0.989) total time=39.4min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=20; accuracy: (test=0.874) roc_auc_ovr: (test=0.992) total time=42.2min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=20; accuracy: (test=0.872) roc_auc_ovr: (test=0.991) total time=48.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=20; accuracy: (test=0.865) roc_auc_ovr: (test=0.988) total time=43.0min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=20; accuracy: (test=0.869) roc_auc_ovr: (test=0.990) total time=36.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=40; accuracy: (test=0.871) roc_auc_ovr: (test=0.988) total time=82.5min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=40; accuracy: (test=0.880) roc_auc_ovr: (test=0.992) total time=90.2min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=40; accuracy: (test=0.884) roc_auc_ovr: (test=0.991) total time=80.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=40; accuracy: (test=0.869) roc_auc_ovr: (test=0.990) total time=84.4min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(1000,), selectpercentile__percentile=40; accuracy: (test=0.882) roc_auc_ovr: (test=0.991) total time=79.2min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=5; accuracy: (test=0.803) roc_auc_ovr: (test=0.979) total time= 1.3min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=5; accuracy: (test=0.804) roc_auc_ovr: (test=0.982) total time= 1.4min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=5; accuracy: (test=0.806) roc_auc_ovr: (test=0.982) total time= 1.2min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=5; accuracy: (test=0.793) roc_auc_ovr: (test=0.978) total time= 1.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=5; accuracy: (test=0.803) roc_auc_ovr: (test=0.979) total time= 1.5min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=10; accuracy: (test=0.836) roc_auc_ovr: (test=0.985) total time= 1.6min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=10; accuracy: (test=0.841) roc_auc_ovr: (test=0.986) total time= 1.8min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=10; accuracy: (test=0.839) roc_auc_ovr: (test=0.987) total time= 1.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=10; accuracy: (test=0.833) roc_auc_ovr: (test=0.985) total time= 2.0min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=10; accuracy: (test=0.844) roc_auc_ovr: (test=0.986) total time= 1.6min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=20; accuracy: (test=0.859) roc_auc_ovr: (test=0.988) total time= 2.6min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=20; accuracy: (test=0.857) roc_auc_ovr: (test=0.990) total time= 3.1min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=20; accuracy: (test=0.863) roc_auc_ovr: (test=0.990) total time= 2.1min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=20; accuracy: (test=0.856) roc_auc_ovr: (test=0.988) total time= 1.9min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=20; accuracy: (test=0.856) roc_auc_ovr: (test=0.988) total time= 2.7min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=40; accuracy: (test=0.865) roc_auc_ovr: (test=0.989) total time= 4.2min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=40; accuracy: (test=0.860) roc_auc_ovr: (test=0.991) total time= 4.7min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=40; accuracy: (test=0.865) roc_auc_ovr: (test=0.991) total time= 3.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=40; accuracy: (test=0.859) roc_auc_ovr: (test=0.989) total time= 4.2min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(50, 25), selectpercentile__percentile=40; accuracy: (test=0.854) roc_auc_ovr: (test=0.990) total time= 5.3min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=5; accuracy: (test=0.820) roc_auc_ovr: (test=0.983) total time= 3.1min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=5; accuracy: (test=0.814) roc_auc_ovr: (test=0.983) total time= 4.9min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=5; accuracy: (test=0.816) roc_auc_ovr: (test=0.983) total time= 3.7min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=5; accuracy: (test=0.815) roc_auc_ovr: (test=0.981) total time= 3.8min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=5; accuracy: (test=0.809) roc_auc_ovr: (test=0.981) total time= 3.9min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=10; accuracy: (test=0.852) roc_auc_ovr: (test=0.987) total time= 3.5min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=10; accuracy: (test=0.841) roc_auc_ovr: (test=0.986) total time= 6.8min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=10; accuracy: (test=0.842) roc_auc_ovr: (test=0.986) total time= 4.9min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=10; accuracy: (test=0.844) roc_auc_ovr: (test=0.986) total time= 5.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=10; accuracy: (test=0.846) roc_auc_ovr: (test=0.987) total time= 5.0min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=20; accuracy: (test=0.874) roc_auc_ovr: (test=0.990) total time= 9.6min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=20; accuracy: (test=0.858) roc_auc_ovr: (test=0.989) total time=18.7min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=20; accuracy: (test=0.867) roc_auc_ovr: (test=0.991) total time=13.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=20; accuracy: (test=0.866) roc_auc_ovr: (test=0.989) total time=14.3min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=20; accuracy: (test=0.860) roc_auc_ovr: (test=0.988) total time=15.9min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=40; accuracy: (test=0.877) roc_auc_ovr: (test=0.991) total time=18.3min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=40; accuracy: (test=0.878) roc_auc_ovr: (test=0.993) total time=25.1min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=40; accuracy: (test=0.865) roc_auc_ovr: (test=0.988) total time=24.7min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=40; accuracy: (test=0.874) roc_auc_ovr: (test=0.990) total time=26.7min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(250, 125), selectpercentile__percentile=40; accuracy: (test=0.879) roc_auc_ovr: (test=0.992) total time=18.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=5; accuracy: (test=0.815) roc_auc_ovr: (test=0.979) total time= 7.9min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=5; accuracy: (test=0.827) roc_auc_ovr: (test=0.985) total time= 4.6min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=5; accuracy: (test=0.825) roc_auc_ovr: (test=0.984) total time= 4.4min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=5; accuracy: (test=0.821) roc_auc_ovr: (test=0.983) total time= 6.1min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=5; accuracy: (test=0.816) roc_auc_ovr: (test=0.983) total time= 6.6min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=10; accuracy: (test=0.839) roc_auc_ovr: (test=0.982) total time=12.4min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=10; accuracy: (test=0.847) roc_auc_ovr: (test=0.987) total time=11.0min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=10; accuracy: (test=0.847) roc_auc_ovr: (test=0.988) total time= 9.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=10; accuracy: (test=0.834) roc_auc_ovr: (test=0.986) total time=10.6min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=10; accuracy: (test=0.838) roc_auc_ovr: (test=0.984) total time=11.8min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=20; accuracy: (test=0.844) roc_auc_ovr: (test=0.986) total time=29.4min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=20; accuracy: (test=0.871) roc_auc_ovr: (test=0.992) total time=18.3min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=20; accuracy: (test=0.845) roc_auc_ovr: (test=0.986) total time=32.0min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=20; accuracy: (test=0.863) roc_auc_ovr: (test=0.989) total time=21.9min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=20; accuracy: (test=0.859) roc_auc_ovr: (test=0.988) total time=22.7min\n",
      "[CV 1/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=40; accuracy: (test=0.885) roc_auc_ovr: (test=0.991) total time=43.9min\n",
      "[CV 2/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=40; accuracy: (test=0.884) roc_auc_ovr: (test=0.993) total time=32.8min\n",
      "[CV 3/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=40; accuracy: (test=0.877) roc_auc_ovr: (test=0.992) total time=28.6min\n",
      "[CV 4/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=40; accuracy: (test=0.878) roc_auc_ovr: (test=0.991) total time=29.5min\n",
      "[CV 5/5] END mlpclassifier__hidden_layer_sizes=(500, 250), selectpercentile__percentile=40; accuracy: (test=0.887) roc_auc_ovr: (test=0.992) total time=29.4min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>percentile</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>mean_test_roc_auc_ovr</th>\n",
       "      <th>std_test_roc_auc_ovr</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(250,)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.886766</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>1193.496633</td>\n",
       "      <td>207.691884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.884485</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.992263</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>288.075506</td>\n",
       "      <td>18.489390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(500, 250)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.882468</td>\n",
       "      <td>0.003801</td>\n",
       "      <td>0.991855</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>1969.442595</td>\n",
       "      <td>343.112819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(500,)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.881831</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.991377</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>2734.831152</td>\n",
       "      <td>520.489281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(1000,)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.877322</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.990539</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>4993.085214</td>\n",
       "      <td>236.257278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(250,)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.877003</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.991127</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>705.027613</td>\n",
       "      <td>76.707757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.875995</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.990932</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>188.560122</td>\n",
       "      <td>25.644004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(250, 125)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.874721</td>\n",
       "      <td>0.004865</td>\n",
       "      <td>0.990987</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>1361.096519</td>\n",
       "      <td>206.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(500,)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.874721</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>0.990609</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>1410.458531</td>\n",
       "      <td>270.969656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(1000,)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.871272</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.989806</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>2519.259795</td>\n",
       "      <td>237.499484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(250, 125)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.864958</td>\n",
       "      <td>0.005647</td>\n",
       "      <td>0.989528</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>863.305296</td>\n",
       "      <td>178.870711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(50, 25)</td>\n",
       "      <td>40</td>\n",
       "      <td>0.860766</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.990063</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>261.851455</td>\n",
       "      <td>34.964815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(50, 25)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.858272</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.988854</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>147.136198</td>\n",
       "      <td>25.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(500, 250)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.856681</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.988117</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>1489.107937</td>\n",
       "      <td>303.989726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(500,)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.856415</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.988032</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>727.893060</td>\n",
       "      <td>114.371637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(250,)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.855248</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.987811</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>430.290133</td>\n",
       "      <td>67.392917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.854611</td>\n",
       "      <td>0.005151</td>\n",
       "      <td>0.987998</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>124.180513</td>\n",
       "      <td>24.028096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(1000,)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.849889</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.986835</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>1525.463015</td>\n",
       "      <td>277.045057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(250, 125)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.844953</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.986426</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>303.135682</td>\n",
       "      <td>62.393201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(500, 250)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.840921</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.985328</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>657.434752</td>\n",
       "      <td>69.460997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(50, 25)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.838693</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>101.024561</td>\n",
       "      <td>10.315558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(500,)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.823145</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.983316</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>440.084649</td>\n",
       "      <td>59.224963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(1000,)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.822031</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.982696</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>880.478537</td>\n",
       "      <td>222.516069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(250,)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.982927</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>259.199362</td>\n",
       "      <td>17.737699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(500, 250)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.820917</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>0.982715</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>351.819044</td>\n",
       "      <td>77.107527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(50,)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.819219</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.982785</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>105.011597</td>\n",
       "      <td>6.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(250, 125)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814868</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.982130</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>230.470133</td>\n",
       "      <td>34.851907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(50, 25)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.801868</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>78.783606</td>\n",
       "      <td>5.307355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_layer_sizes percentile  mean_test_accuracy  std_test_accuracy  \\\n",
       "7              (250,)         40            0.886766           0.003797   \n",
       "3               (50,)         40            0.884485           0.002196   \n",
       "27         (500, 250)         40            0.882468           0.003801   \n",
       "11             (500,)         40            0.881831           0.007362   \n",
       "15            (1000,)         40            0.877322           0.005955   \n",
       "6              (250,)         20            0.877003           0.004383   \n",
       "2               (50,)         20            0.875995           0.002430   \n",
       "23         (250, 125)         40            0.874721           0.004865   \n",
       "10             (500,)         20            0.874721           0.003721   \n",
       "14            (1000,)         20            0.871272           0.003765   \n",
       "22         (250, 125)         20            0.864958           0.005647   \n",
       "19           (50, 25)         40            0.860766           0.004349   \n",
       "18           (50, 25)         20            0.858272           0.002732   \n",
       "26         (500, 250)         20            0.856681           0.010442   \n",
       "9              (500,)         10            0.856415           0.003206   \n",
       "5              (250,)         10            0.855248           0.004232   \n",
       "1               (50,)         10            0.854611           0.005151   \n",
       "13            (1000,)         10            0.849889           0.005522   \n",
       "21         (250, 125)         10            0.844953           0.003861   \n",
       "25         (500, 250)         10            0.840921           0.005231   \n",
       "17           (50, 25)         10            0.838693           0.003771   \n",
       "8              (500,)          5            0.823145           0.005638   \n",
       "12            (1000,)          5            0.822031           0.005353   \n",
       "4              (250,)          5            0.821342           0.003294   \n",
       "24         (500, 250)          5            0.820917           0.004889   \n",
       "0               (50,)          5            0.819219           0.003822   \n",
       "20         (250, 125)          5            0.814868           0.003662   \n",
       "16           (50, 25)          5            0.801868           0.004367   \n",
       "\n",
       "    mean_test_roc_auc_ovr  std_test_roc_auc_ovr  mean_fit_time  std_fit_time  \n",
       "7                0.992452              0.000662    1193.496633    207.691884  \n",
       "3                0.992263              0.000761     288.075506     18.489390  \n",
       "27               0.991855              0.000865    1969.442595    343.112819  \n",
       "11               0.991377              0.001358    2734.831152    520.489281  \n",
       "15               0.990539              0.001262    4993.085214    236.257278  \n",
       "6                0.991127              0.000996     705.027613     76.707757  \n",
       "2                0.990932              0.000830     188.560122     25.644004  \n",
       "23               0.990987              0.001578    1361.096519    206.847500  \n",
       "10               0.990609              0.001289    1410.458531    270.969656  \n",
       "14               0.989806              0.001314    2519.259795    237.499484  \n",
       "22               0.989528              0.000901     863.305296    178.870711  \n",
       "19               0.990063              0.001029     261.851455     34.964815  \n",
       "18               0.988854              0.000803     147.136198     25.281190  \n",
       "26               0.988117              0.002002    1489.107937    303.989726  \n",
       "9                0.988032              0.000559     727.893060    114.371637  \n",
       "5                0.987811              0.001201     430.290133     67.392917  \n",
       "1                0.987998              0.001404     124.180513     24.028096  \n",
       "13               0.986835              0.002378    1525.463015    277.045057  \n",
       "21               0.986426              0.000515     303.135682     62.393201  \n",
       "25               0.985328              0.002012     657.434752     69.460997  \n",
       "17               0.985915              0.000869     101.024561     10.315558  \n",
       "8                0.983316              0.001025     440.084649     59.224963  \n",
       "12               0.982696              0.001950     880.478537    222.516069  \n",
       "4                0.982927              0.001233     259.199362     17.737699  \n",
       "24               0.982715              0.002063     351.819044     77.107527  \n",
       "0                0.982785              0.000895     105.011597      6.154200  \n",
       "20               0.982130              0.000892     230.470133     34.851907  \n",
       "16               0.980100              0.001608      78.783606      5.307355  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "  TfidfVectorizer(stop_words='english', max_features=100_000),\n",
    "  SelectPercentile(chi2, percentile=10),\n",
    "  MaxAbsScaler(),\n",
    "  MLPClassifier()\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "  'selectpercentile__percentile': [5, 10, 20, 40],\n",
    "  'mlpclassifier__hidden_layer_sizes': [(50,), (250,), (500,), (1000,), (50, 25), (250, 125), (500, 250)]\n",
    "}\n",
    "\n",
    "mlp_grid_search = GridSearchCV(pipeline, param_grid, scoring=('accuracy', 'roc_auc_ovr'), refit='accuracy', cv=5, verbose=3)\n",
    "\n",
    "mlp_grid_search.fit(df['text'], df['y'])\n",
    "\n",
    "columns_rename = {\n",
    "  'param_mlpclassifier__hidden_layer_sizes': 'hidden_layer_sizes',\n",
    "  'param_selectpercentile__percentile': 'percentile'\n",
    "}\n",
    "columns_to_select = [\n",
    "  'hidden_layer_sizes',\n",
    "  'percentile',\n",
    "  'mean_test_accuracy',\n",
    "  'std_test_accuracy',\n",
    "  'mean_test_roc_auc_ovr',\n",
    "  'std_test_roc_auc_ovr',\n",
    "  'mean_fit_time',\n",
    "  'std_fit_time'\n",
    "]\n",
    "scores_df = pd.DataFrame(mlp_grid_search.cv_results_).rename(columns=columns_rename)\n",
    "scores_df[columns_to_select].sort_values(by=['mean_test_accuracy'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that took longer than expected. In the end, we get 0.01-0.02 accuracy increase over NB and LR. It's funny how 1000 neurons didn't give the best results, I assumed it would and that it would overfit, but in reality it just didn't deliver. We can also see how the length of a vector used directly affects the performance of the network. That being said, I'm not sure anyone actually uses sparse vectors for NN models, so now is the time for dense vectors. Let's start with a pretrained word2vec, I will use a 50-unit layer as it seems to provide good results while still being relatively quick to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec =  gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded word2vec in a separate widget so that it would stay in RAM (otherwise, it takes damn 40 secs to load every time). Now let's prepare the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.018354833, 0.01911231, -0.003590454, 0.0425...\n",
       "1    [-0.01658663, -0.0056632212, 0.0072250823, 0.0...\n",
       "2    [-0.00084048323, 0.004721938, 0.018587181, 0.0...\n",
       "3    [-0.031490926, -0.006962086, 0.0083416365, 0.0...\n",
       "4    [0.02332852, -0.0047870562, 0.022667598, 0.035...\n",
       "Name: mean_word2vec, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "def text2word2vec(text):\n",
    "  tokenized_text =  [w for w in word_tokenize(text) if w not in eng_stopwords]\n",
    "  return word2vec.get_mean_vector(tokenized_text) if len(tokenized_text) else None\n",
    "\n",
    "df['mean_word2vec'] = df['text'].apply(text2word2vec)\n",
    "\n",
    "df['mean_word2vec'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_word2vec'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df['mean_word2vec'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got rid of some None vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18812, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# god knows why this bs works but I already spent way too much time to find the answer\n",
    "# without this df['mean_word2vec'] has the shape (18812,) and sklearn refuses to take it\n",
    "mean_word2vec_matrix = np.array(np.concatenate(df[['mean_word2vec']].values).tolist())\n",
    "mean_word2vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.716) roc_auc_ovr: (test=0.963) total time=  29.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.722) roc_auc_ovr: (test=0.962) total time=  28.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   57.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.720) roc_auc_ovr: (test=0.963) total time=  31.1s\n",
      "[CV] END ... accuracy: (test=0.715) roc_auc_ovr: (test=0.963) total time=  29.2s\n",
      "[CV] END ... accuracy: (test=0.713) roc_auc_ovr: (test=0.963) total time=  54.6s\n",
      "0.7172547749760779±0.0030662066612993206\n",
      "0.9628886928505217±0.0003607229761157865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\n",
    "\n",
    "cv_scores = cross_validate(mlp, mean_word2vec_matrix, df['y'], cv=5, scoring=('accuracy', 'roc_auc_ovr'), verbose=3)\n",
    "\n",
    "print(f\"{cv_scores['test_accuracy'].mean()}±{cv_scores['test_accuracy'].std()}\")\n",
    "print(f\"{cv_scores['test_roc_auc_ovr'].mean()}±{cv_scores['test_roc_auc_ovr'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.720) roc_auc_ovr: (test=0.965) total time=  32.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   32.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.724) roc_auc_ovr: (test=0.964) total time=  46.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ... accuracy: (test=0.731) roc_auc_ovr: (test=0.964) total time=  45.8s\n",
      "[CV] END ... accuracy: (test=0.717) roc_auc_ovr: (test=0.963) total time=  41.6s\n",
      "[CV] END ... accuracy: (test=0.719) roc_auc_ovr: (test=0.963) total time=  39.8s\n",
      "0.7220923022411196±0.005135455919028041\n",
      "0.9637498152252227±0.0007827808913493769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.4min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "mean_word2vec_matrix_normalized = standardScaler.fit_transform(mean_word2vec_matrix)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\n",
    "\n",
    "cv_scores = cross_validate(mlp, mean_word2vec_matrix_normalized, df['y'], cv=5, scoring=('accuracy', 'roc_auc_ovr'), verbose=3)\n",
    "\n",
    "print(f\"{cv_scores['test_accuracy'].mean()}±{cv_scores['test_accuracy'].std()}\")\n",
    "print(f\"{cv_scores['test_roc_auc_ovr'].mean()}±{cv_scores['test_roc_auc_ovr'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty bad. I feel like there's some serious mistake in my reasoning or code as the score seems to be way too low. Maybe I should train word2vec myself? AFAIK averaged word2vec vector is pretty similar to doc2vec in terms of performance. Let's quickly try doc2vec to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.35365555, 0.17339046, 0.18393995, -0.58620...\n",
       "1    [-0.2553672, 0.12558939, 0.15981369, -0.221346...\n",
       "2    [0.29246834, -0.16016297, -0.13159648, 1.15566...\n",
       "3    [-0.29304528, -0.7467441, 0.30352396, -0.75571...\n",
       "4    [-0.21903262, 0.11049359, 0.004415522, -0.1185...\n",
       "Name: doc2vec, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "documents = [TaggedDocument(word_tokenize(doc), [i]) for i, doc in enumerate(df['text'])]\n",
    "\n",
    "doc2vec = Doc2Vec(documents, min_count=10, workers=8)\n",
    "\n",
    "df['doc2vec'] = df['text'].apply(lambda doc: doc2vec.infer_vector(word_tokenize(doc)))\n",
    "\n",
    "df['doc2vec'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc2vec_matrix = np.array(np.concatenate(df[['doc2vec']].values).tolist())\n",
    "doc2vec_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6019313216220838±0.005598206367794166\n",
      "0.9473297495737375±0.001923384963970542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\n",
    "\n",
    "cv_scores = cross_validate(mlp, doc2vec_matrix, df['y'], cv=5, scoring=('accuracy', 'roc_auc_ovr'))\n",
    "\n",
    "print(f\"{cv_scores['test_accuracy'].mean()}±{cv_scores['test_accuracy'].std()}\")\n",
    "print(f\"{cv_scores['test_roc_auc_ovr'].mean()}±{cv_scores['test_roc_auc_ovr'].std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5846860011837459±0.008096889133146996\n",
      "0.941561599511474±0.0013361609949245645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real_marshal/Projects/newt/.venv/lib64/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:679: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "doc2vec_matrix_normalized = standardScaler.fit_transform(doc2vec_matrix)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\n",
    "\n",
    "cv_scores = cross_validate(mlp, doc2vec_matrix_normalized, df['y'], cv=5, scoring=('accuracy', 'roc_auc_ovr'))\n",
    "\n",
    "print(f\"{cv_scores['test_accuracy'].mean()}±{cv_scores['test_accuracy'].std()}\")\n",
    "print(f\"{cv_scores['test_roc_auc_ovr'].mean()}±{cv_scores['test_roc_auc_ovr'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, fuck it, let's just forget everything now and take spacy with the default text classification config and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "db = DocBin()\n",
    "\n",
    "categories = df['y_text'].unique()\n",
    "\n",
    "def prepare_data(X, y, filename):\n",
    "  for text, category in zip(X, y):\n",
    "    doc = nlp.make_doc(text)\n",
    "    doc.cats = {category: 0 for category in categories}\n",
    "    doc.cats[category] = 1\n",
    "    db.add(doc)\n",
    "\n",
    "  db.to_disk(f'{filename}.spacy')\n",
    "\n",
    "df_train = df.sample(frac=0.7)\n",
    "df_dev_test = df.drop(df_train.index)\n",
    "df_dev = df.sample(frac=0.5)\n",
    "df_test = df.drop(df_dev.index)\n",
    "\n",
    "prepare_data(df_train['text'], df_train['y_text'], 'train')\n",
    "prepare_data(df_dev['text'], df_dev['y_text'], 'dev')\n",
    "prepare_data(df_test['text'], df_test['y_text'], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config --pipeline textcat -G -F config.cfg\n",
    "!python -m spacy train config.cfg --paths.train ./train.spacy  --paths.dev ./dev.spacy --output spacy_model\n",
    "!python -m spacy evaluate spacy_model/model-best/ --output metrics.json ./test.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after a few hours we get macro f-score of 0.93... 💀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's funny is that the default config doesn't seem to use anything complex: CPU accuracy one uses an ensemble of some linear BoW model and a CNN model, GPU accuracy one is the same but it also incorporates transformers, while the one I used looks almost like an LR with a simple counting BoW (the docs suck btw, had to look at the source). But then again, why does it take so long to train such a simple model? Which extra hyperparameters do they update?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to dig deeper into spacy now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88e5c33f56a08cc8848567727c0cfe7b31991892ec15cd7489b89c8eb28d72ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
